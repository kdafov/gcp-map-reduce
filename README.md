# Introduction
 
Google Cloud Platform services running Map-Reduce model to find all sets of anagrams from 500 books. Details of how GCP products are configured along with their purpose, and detailed explanation of the implementation for the mapper, shuffler and reducer. A runtime report showing the time and memory-consumption of each service is also listed along with pricing estimates.

# Services & Configuration

The following GCP Services are requried for this model:
- ***3 Cloud functions*** (region ***us-central1*** runtime ***Python >3.10***)
  - ***readBucket***\
    trigger: HTTP\
    memory: 128MB\
    timeout: 60s\
    maximum instances: 1
  - ***mapper***\
    trigger: pub/sub (book-subscription)\
    memory: 256MB\
    timeout: 60s\
    maximum instances: 100
  - ***reduce***\
    trigger: pub/sub (book-subscription)\
    memory: 1GB\
    timeout: 120s\
    maximum instances: 100
- ***2 Pub/Sub Systems***
  - ***book-subscription***
  - ***reducer-subscription***
- ***2 Cloud Storage Buckets*** (location ***US*** storage-class ***Standard***)
  - ***shuffler-output*** (Public access prevention)
  - ***anagrams*** (Public)

# Purpose of services

This section will explain the functionality of each GCP service and what is its purpose in the Map-Reducer model.

There are total of 3 cloud functions used. Each cloud function is detailed below together with implementation techniques used to reduce the overall computational time and improve the efficiency of the Map-Reducer model.

## readBucket

This cloud function is the starting point to the Map-Reducer model. Once triggered, it will start reading the content of the bucket provided (***CONFIDENTIAL***) and send a message of the file name for each file to the book-subscription Pub/Sub. Additionally, this function will wipe the content of the bucket that is storing the shufflers outputs from any previous runs (gs://shuffler-output).

## mapper

This cloud function will be triggered every time there is a new message send to the book-subscription Pub/Sub. When triggered the function will read the message, which will contain the filename of the book, and will read the content of that file from the ***CONFIDENTIAL*** bucket. The raw file content will then go through multiple steps of data cleaning to ensure that the data is in the smallest possible set before it gets processed to reduce computational time; Cleaning procedure includes removing whitespace, transforming dashes to spaces (re-use to produce two words re and use), removing punctuation, removing numbers and roman numbers (XVII, IV, X, …), removing duplicates, removing stop words (https://www.textfixer.com/tutorials/common-english-words-with-contractions.txt), and removing single character letters (a, b, c, …). Once the file content is cleaned it will be forwarded to the mapper which will produce key-value pairs for every word in the file and will save all key-value pairs to a list. To illustrate how this works:\
![image](https://github.com/kdafov/gcp-map-reduce/assets/94061728/581a11de-a9d7-4b4c-9696-740c15d5b27f)

## shuffler

The shuffler function is contained within the mapper cloud function. The function will get the list of key-value pairs generated by the mapper and will group them by keys producing a dictionary with key-list pairs. From the above example, we will get the following:\
![image](https://github.com/kdafov/gcp-map-reduce/assets/94061728/24bc7bdd-8a97-4033-b426-ac219feeed5b)

After the shuffler has compiled a dictionary of key-list pairs, it will save them to ***shuffler-output*** bucket as ***bookname.txt*** and a message will be sent to the reducer-subscription Pub/Sub, which will inform the Reducer that there is a new shuffler output file in the bucket, and that it should recompile the anagrams list. Dictionary comprehensions used throughout as there are highly efficient for large volumes of data.

## reducer

The reducer cloud function will trigger when there is a new message sent to the reducer-subscription Pub/Sub. Since a new message will be sent every time there is a new shuffler output, this can cause the reducer function to run many instances (~45) and they will all try to recompile the anagrams list, causing huge memory consumption and high costs. To avoid this I have implemented the concept of semaphores, where only one reducer function will be able to recompile at a time, which reduces instances to about 8 at a time (-82%). When the reducer function is executed, it will check if the semaphore is unlocked and if not, it will terminate; If yes – it will read all shuffler outputs from the shuffler-output bucket and will store each dictionary in a list. It will then use complex dictionary comprehensions to group all dictionaries in the list by keys, remove duplicate values and remove pairs that have less than 2 items (as per the requirements). It will then store the values in a list (alphabetically sorted) and will organize them by number of anagrams from small to large. A message of how many anagrams are found will be logged in the logs and a final output of the anagrams list will be stored in ***CONFIDENTIAL***/anagrams bucket. Once function terminates, the semaphore will be unlocked.

## book-subscription Pub/Sub

Pub/Sub used to start an instance of the Mapper/Shuffler function for given book file.

## reducer-subscription Pub/Sub

Pub/Sub used to start an instance of the Reducer function i.e., to inform the Reducer that there is a new shuffler file in the bucket and that the anagrams should be recalculated.

# Execution time & Cost

| Function name | Average execution time (ms) | Memory (MiB) | Cost ($ USD / run) | Average instances |
|---------------|----------|----------|----------|----------|
| readBucket | 22699 | 120.35 | 0.0002265 | 1 |
| mapper | 59876 | 144.27 | 0.0202079 | 37 |
| reducer | 98952 | 495.86 | 0.0074298 | 13 |

This gives us an average execution time of 181 527ms (3m 1s). Additionally the estimated costs for the book-subscription Pub/Sub, reducer-subscription Pub/Sub, shuffler-output Storage Bucket and
anagrams Storage Bucket are $0.000000001, $0.00000002, $0.000015 and $0.000000133 respectively. The MapReduce model average cost is ***$0.0279*** per run per 500 books.
